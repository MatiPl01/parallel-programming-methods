{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR58VpbD3NwI",
        "outputId": "bc309443-2e56-4934-c4cd-87844aa3c4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun 23 06:08:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, zipfile, glob, pathlib, shutil\n",
        "\n",
        "# Automatic detection of the first *.zip in /content\n",
        "zip_candidates = glob.glob('/content/*.zip')\n",
        "if zip_candidates:\n",
        "    zip_path = zip_candidates[0]\n",
        "else:\n",
        "    raise FileNotFoundError(\"Nie znaleziono pliku .zip ‑ wgraj CUDA‑Lab02‑2025.zip do /content\")\n",
        "\n",
        "print(\"Using:\", zip_path)\n",
        "\n",
        "extract_root = '/content/cuda_lab02'\n",
        "if os.path.exists(extract_root):\n",
        "    shutil.rmtree(extract_root)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(extract_root)\n",
        "\n",
        "print(\"Rozpakowano do:\", extract_root)\n",
        "!which tree || (apt-get update -qq && apt-get install -y tree)\n",
        "!tree -L 2 -F {extract_root} | head -n 40\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H9fz6V93lwg",
        "outputId": "db24f639-4acf-484f-ad73-6dca3a960472"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: /content/CUDA-Lab02-2025.zip\n",
            "Rozpakowano do: /content/cuda_lab02\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 0s (180 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 126319 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "/content/cuda_lab02/\n",
            "├── 1 Reduction/\n",
            "│   ├── analyser.bat\n",
            "│   ├── exception.h\n",
            "│   ├── helper_timer.h\n",
            "│   ├── profiler.bat\n",
            "│   ├── reduction_global.cpp\n",
            "│   ├── reduction_global_gpu.bat\n",
            "│   ├── reduction_global_kernel.cu\n",
            "│   ├── reduction.h\n",
            "│   ├── reduction_shared.cpp\n",
            "│   └── reduction_shared_kernel.cu\n",
            "├── 2 Warp Divergence/\n",
            "│   ├── reduction.cpp\n",
            "│   ├── reduction.h\n",
            "│   ├── reduction_kernel_interleaving.cu\n",
            "│   └── reduction_kernel_sequential.cu\n",
            "├── 3 Loop Unrolling/\n",
            "│   ├── reduction_cg_kernel.cu\n",
            "│   ├── reduction.cpp\n",
            "│   ├── reduction.h\n",
            "│   └── reduction_wp_kernel.cu\n",
            "├── 4 Atomic Operations/\n",
            "│   ├── reduction_blk_atmc_kernel.cu\n",
            "│   ├── reduction.cpp\n",
            "│   ├── reduction.h\n",
            "│   ├── reduction_kernel.cu\n",
            "│   └── reduction_wrp_atmc_kernel.cu\n",
            "├── 5 Histogram/\n",
            "│   ├── gputimer.h\n",
            "│   └── histo.cu\n",
            "├── CUDA-Lab02-zadania.pdf\n",
            "├── exception.h\n",
            "└── helper_timer.h\n",
            "\n",
            "5 directories, 28 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKZB_5Gh33D_",
        "outputId": "611d7044-1021-4632-8cb3-4795345a79d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAe6Gz6B5QEn",
        "outputId": "c4e5a4c8-1c1a-48a7-9435-e1188bb4dbcb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpjr8pw64j\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Redukcja – globalna vs. pamięć współdzielona\n",
        "\n",
        "Porównujemy dwa warianty:\n",
        "\n",
        "* **Global memory** – pliki `reduction_global.*`\n",
        "* **Shared memory** – pliki `reduction_shared.*`\n",
        "\n",
        "Każdy program:\n",
        "\n",
        "1. generuje losowy wektor `N = 1 << 24` (≈ 16,8 M),\n",
        "2. wykonuje `test_iter = 30` redukcji na GPU,\n",
        "3. wypisuje średni czas na iterację i przepustowość.\n",
        "\n",
        "> Źródło: *folder `1 Reduction/…` w archiwum*.\n"
      ],
      "metadata": {
        "id": "0yGslT9E555a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "cd \"/content/cuda_lab02/1 Reduction\"\n",
        "\n",
        "ARCH=\"sm_75\"        # dla Tesli T4; sprawdź `!nvidia-smi` i zmień w razie potrzeby\n",
        "\n",
        "echo \"== Kompilacja ==\"\n",
        "nvcc -O3 -arch=${ARCH} -I. reduction_global.cpp  reduction_global_kernel.cu  -o reduction_global\n",
        "nvcc -O3 -arch=${ARCH} -I. reduction_shared.cpp  reduction_shared_kernel.cu -o reduction_shared\n",
        "\n",
        "echo\n",
        "echo \"== Global memory reduction ==\"\n",
        "./reduction_global\n",
        "\n",
        "echo\n",
        "echo \"== Shared memory reduction ==\"\n",
        "./reduction_shared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmgz_ozw5YI2",
        "outputId": "0290222e-e607-4980-f30e-0d549d16705b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Kompilacja ==\n",
            "\n",
            "== Global memory reduction ==\n",
            "Time= 14.355 msec, bandwidth= 4.674908 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "== Shared memory reduction ==\n",
            "Time= 1.590 msec, bandwidth= 42.203384 GB/s\n",
            "host: 0.996007, device 0.996007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Komentarz:** Jak pokazują powyższe wyniki, wariant z pamięcią współdzieloną\n",
        "> osiąga zauważalnie większą przepustowość, ponieważ eliminuje wielokrotne\n",
        "> odwołania do pamięci globalnej w trakcie drzewiastej redukcji.  \n",
        "> Dodatkowo `shared` ogranicza liczbę transakcji pamięciowych dzięki koaleskowaniu\n",
        "> wątków wewnątrz bloku.\n"
      ],
      "metadata": {
        "id": "-AnRIw5y5_aK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Warp divergence – *sequential* vs. *interleaved*\n",
        "\n",
        "Redukcję z punktu 1 implementujemy na dwa sposoby różniące się **kolejnością\n",
        "identyfikatorów wątków**:\n",
        "\n",
        "* `reduction_kernel_sequential.cu`\n",
        "* `reduction_kernel_interleaving.cu`\n",
        "\n",
        "Spodziewamy się zbliżonych czasów; divergence nie gra dużej roli, ponieważ każda\n",
        "warstwa redukcji synchronizuje wątki w obrębie warpu.\n"
      ],
      "metadata": {
        "id": "TJJVpi1c6B8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "cd \"/content/cuda_lab02/2 Warp Divergence\"\n",
        "\n",
        "ARCH=\"sm_75\"   # Tesla T4; sprawdź `!nvidia-smi`, zmień jeśli trzeba\n",
        "\n",
        "echo \"== Kompilacja wariantu SEQUENTIAL ==\"\n",
        "nvcc -O3 -arch=${ARCH} -I. -I.. \\\n",
        "     reduction.cpp \\\n",
        "     reduction_kernel_sequential.cu \\\n",
        "     -o reduction_div_seq\n",
        "\n",
        "echo\n",
        "echo \"== Kompilacja wariantu INTERLEAVING ==\"\n",
        "nvcc -O3 -arch=${ARCH} -I. -I.. \\\n",
        "     reduction.cpp \\\n",
        "     reduction_kernel_interleaving.cu \\\n",
        "     -o reduction_div_int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1wQyIPQ5iRg",
        "outputId": "74c40256-3365-4e62-a86f-ecf1e16d0568"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Kompilacja wariantu SEQUENTIAL ==\n",
            "\n",
            "== Kompilacja wariantu INTERLEAVING ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd \"/content/cuda_lab02/2 Warp Divergence\"\n",
        "\n",
        "echo \"== Uruchamiamy SEQUENTIAL ==\"\n",
        "./reduction_div_seq\n",
        "\n",
        "echo\n",
        "echo \"== Uruchamiamy INTERLEAVING ==\"\n",
        "./reduction_div_int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDZZggsC6eZs",
        "outputId": "d42986b5-70bf-4684-81ac-d114145b01d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Uruchamiamy SEQUENTIAL ==\n",
            "Time= 2.478 msec, bandwidth= 27.081863 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "== Uruchamiamy INTERLEAVING ==\n",
            "Time= 3.012 msec, bandwidth= 22.280499 GB/s\n",
            "host: 0.996007, device 0.996007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Obserwacja:** Różnica jest niewielka.  \n",
        "> Divergence został ograniczony do pojedynczego warpu i nie dominuje czasu\n",
        "> wykonania – większość cykli to pobieranie danych z pamięci.\n"
      ],
      "metadata": {
        "id": "qJrv8edU6Lkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd \"/content/cuda_lab02/2 Warp Divergence\"\n",
        "\n",
        "echo \"== Profiluję wariant SEQUENTIAL ==\"\n",
        "nvprof ./reduction_div_seq\n",
        "\n",
        "echo\n",
        "echo \"== Profiluję wariant INTERLEAVING ==\"\n",
        "nvprof ./reduction_div_int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJLyxPmi6EAi",
        "outputId": "4523849d-483e-43d7-db1d-cff9beff6985"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Profiluję wariant SEQUENTIAL ==\n",
            "Time= 2.575 msec, bandwidth= 26.059669 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "== Profiluję wariant INTERLEAVING ==\n",
            "Time= 3.060 msec, bandwidth= 21.932436 GB/s\n",
            "host: 0.996007, device 0.996007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==3578== NVPROF is profiling process 3578, command: ./reduction_div_seq\n",
            "==3578== Profiling application: ./reduction_div_seq\n",
            "==3578== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   57.92%  17.678ms         1  17.678ms  17.678ms  17.678ms  [CUDA memcpy HtoD]\n",
            "                   33.22%  10.140ms        16  633.73us  4.7680us  1.6770ms  reduction_kernel_2(float*, float*, unsigned int)\n",
            "                    8.85%  2.7009ms         5  540.17us  539.61us  540.79us  [CUDA memcpy DtoD]\n",
            "                    0.01%  2.1120us         1  2.1120us  2.1120us  2.1120us  [CUDA memcpy DtoH]\n",
            "      API calls:   84.96%  189.75ms         2  94.873ms  146.00us  189.60ms  cudaMalloc\n",
            "                    8.05%  17.985ms         7  2.5693ms  8.0630us  17.889ms  cudaMemcpy\n",
            "                    5.68%  12.686ms         1  12.686ms  12.686ms  12.686ms  cudaDeviceSynchronize\n",
            "                    1.11%  2.4688ms         2  1.2344ms  229.16us  2.2397ms  cudaFree\n",
            "                    0.13%  296.40us        16  18.524us  5.9120us  180.39us  cudaLaunchKernel\n",
            "                    0.06%  139.07us       114  1.2190us     110ns  56.364us  cuDeviceGetAttribute\n",
            "                    0.01%  13.396us         1  13.396us  13.396us  13.396us  cuDeviceGetName\n",
            "                    0.00%  4.6330us         1  4.6330us  4.6330us  4.6330us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.5290us         2  1.2640us     226ns  2.3030us  cuDeviceGet\n",
            "                    0.00%  1.2350us         3     411ns     123ns     854ns  cuDeviceGetCount\n",
            "                    0.00%     404ns         1     404ns     404ns     404ns  cuDeviceTotalMem\n",
            "                    0.00%     344ns         1     344ns     344ns     344ns  cuModuleGetLoadingMode\n",
            "                    0.00%     199ns         1     199ns     199ns     199ns  cuDeviceGetUuid\n",
            "==3593== NVPROF is profiling process 3593, command: ./reduction_div_int\n",
            "==3593== Profiling application: ./reduction_div_int\n",
            "==3593== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   47.84%  14.007ms         1  14.007ms  14.007ms  14.007ms  [CUDA memcpy HtoD]\n",
            "                   42.93%  12.572ms        16  785.75us  5.0240us  2.0807ms  reduction_kernel_1(float*, float*, unsigned int)\n",
            "                    9.22%  2.7000ms         5  540.01us  539.42us  540.70us  [CUDA memcpy DtoD]\n",
            "                    0.01%  2.2720us         1  2.2720us  2.2720us  2.2720us  [CUDA memcpy DtoH]\n",
            "      API calls:   83.82%  161.09ms         2  80.545ms  104.50us  160.99ms  cudaMalloc\n",
            "                    7.90%  15.185ms         1  15.185ms  15.185ms  15.185ms  cudaDeviceSynchronize\n",
            "                    7.40%  14.231ms         7  2.0330ms  4.9130us  14.172ms  cudaMemcpy\n",
            "                    0.65%  1.2576ms         2  628.78us  144.14us  1.1134ms  cudaFree\n",
            "                    0.11%  220.30us        16  13.768us  3.6880us  147.54us  cudaLaunchKernel\n",
            "                    0.09%  176.09us       114  1.5440us     144ns  69.301us  cuDeviceGetAttribute\n",
            "                    0.01%  15.055us         1  15.055us  15.055us  15.055us  cuDeviceGetName\n",
            "                    0.00%  4.3430us         1  4.3430us  4.3430us  4.3430us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.6520us         3     550ns     200ns  1.1370us  cuDeviceGetCount\n",
            "                    0.00%     771ns         2     385ns     136ns     635ns  cuDeviceGet\n",
            "                    0.00%     633ns         1     633ns     633ns     633ns  cuDeviceTotalMem\n",
            "                    0.00%     524ns         1     524ns     524ns     524ns  cuModuleGetLoadingMode\n",
            "                    0.00%     416ns         1     416ns     416ns     416ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Loop Unrolling – cooperative groups vs. warp primitives\n",
        "\n",
        "Redukcja wykonywana dwoma sposobami:\n",
        "* **CG** – kernel `reduction_cg_kernel.cu`, synchronizacja przez *cooperative groups*,\n",
        "* **WP** – kernel `reduction_wp_kernel.cu`, shuffle / warp primitives.\n",
        "\n",
        "Porównujemy czasy **z** i **bez** `#pragma unroll`."
      ],
      "metadata": {
        "id": "H6pHM-AGbtdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "cd \"/content/cuda_lab02/3 Loop Unrolling\"\n",
        "\n",
        "ARCH=\"sm_75\"\n",
        "CFLAGS=\"-O3 -arch=${ARCH} -I. -I..\"\n",
        "\n",
        "echo \"== Kompilacja z UNROLL (CG) ==\"\n",
        "nvcc $CFLAGS reduction.cpp reduction_cg_kernel.cu -o red_cg_unroll\n",
        "echo\n",
        "echo \"== Kompilacja z UNROLL (WP) ==\"\n",
        "nvcc $CFLAGS reduction.cpp reduction_wp_kernel.cu -o red_wp_unroll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdUil06i6mxp",
        "outputId": "22fcd5cb-5874-480f-efd4-0486036addae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Kompilacja z UNROLL (CG) ==\n",
            "\n",
            "== Kompilacja z UNROLL (WP) ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# tworzymy tymczasowe wersje bez unroll\n",
        "set -e\n",
        "cd \"/content/cuda_lab02/3 Loop Unrolling\"\n",
        "\n",
        "sed 's/#pragma[[:space:]]\\\\+unroll/#pragma unroll 1/' reduction_cg_kernel.cu > tmp_cg_nounroll.cu\n",
        "sed 's/#pragma[[:space:]]\\\\+unroll/#pragma unroll 1/' reduction_wp_kernel.cu > tmp_wp_nounroll.cu\n",
        "\n",
        "ARCH=\"sm_75\"\n",
        "CFLAGS=\"-O3 -arch=${ARCH} -I. -I..\"\n",
        "\n",
        "echo \"== Kompilacja BEZ UNROLL (CG) ==\"\n",
        "nvcc $CFLAGS reduction.cpp tmp_cg_nounroll.cu -o red_cg_nounroll\n",
        "echo\n",
        "echo \"== Kompilacja BEZ UNROLL (WP) ==\"\n",
        "nvcc $CFLAGS reduction.cpp tmp_wp_nounroll.cu -o red_wp_nounroll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZXrQd8bbv-m",
        "outputId": "03832df5-843f-4cf3-9c83-60e97759bcc9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Kompilacja BEZ UNROLL (CG) ==\n",
            "\n",
            "== Kompilacja BEZ UNROLL (WP) ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd \"/content/cuda_lab02/3 Loop Unrolling\"\n",
        "\n",
        "echo \"🔹 CG – UNROLL ON\"\n",
        "nvprof --print-gpu-summary ./red_cg_unroll\n",
        "echo\n",
        "echo \"🔹 CG – UNROLL OFF\"\n",
        "nvprof --print-gpu-summary ./red_cg_nounroll\n",
        "\n",
        "echo\n",
        "echo \"🔹 WP – UNROLL ON\"\n",
        "nvprof --print-gpu-summary ./red_wp_unroll\n",
        "echo\n",
        "echo \"🔹 WP – UNROLL OFF\"\n",
        "nvprof --print-gpu-summary ./red_wp_nounroll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXfYs9Q_bzrK",
        "outputId": "7a074353-fd76-4bee-f596-53602f70becb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 CG – UNROLL ON\n",
            "Time= 0.814 msec, bandwidth= 82.446358 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "🔹 CG – UNROLL OFF\n",
            "Time= 0.814 msec, bandwidth= 82.457504 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "🔹 WP – UNROLL ON\n",
            "Time= 0.808 msec, bandwidth= 83.048325 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "🔹 WP – UNROLL OFF\n",
            "Time= 0.802 msec, bandwidth= 83.656021 GB/s\n",
            "host: 0.996007, device 0.996007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==5072== NVPROF is profiling process 5072, command: ./red_cg_unroll\n",
            "==5072== Profiling application: ./red_cg_unroll\n",
            "==5072== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   56.76%  54.059ms       100  540.59us  539.87us  541.47us  [CUDA memcpy DtoD]\n",
            "                   28.30%  26.950ms       202  133.42us  4.5430us  265.25us  reduction_kernel(float*, float*, unsigned int)\n",
            "                   14.94%  14.228ms         1  14.228ms  14.228ms  14.228ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  2.2400us         1  2.2400us  2.2400us  2.2400us  [CUDA memcpy DtoH]\n",
            "==5087== NVPROF is profiling process 5087, command: ./red_cg_nounroll\n",
            "==5087== Profiling application: ./red_cg_nounroll\n",
            "==5087== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   56.76%  54.057ms       100  540.57us  539.80us  541.12us  [CUDA memcpy DtoD]\n",
            "                   28.28%  26.935ms       202  133.34us  4.5430us  265.31us  reduction_kernel(float*, float*, unsigned int)\n",
            "                   14.95%  14.238ms         1  14.238ms  14.238ms  14.238ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  2.2400us         1  2.2400us  2.2400us  2.2400us  [CUDA memcpy DtoH]\n",
            "==5102== NVPROF is profiling process 5102, command: ./red_wp_unroll\n",
            "==5102== Profiling application: ./red_wp_unroll\n",
            "==5102== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   56.54%  53.492ms       100  534.92us  534.27us  535.52us  [CUDA memcpy DtoD]\n",
            "                   28.47%  26.936ms       202  133.35us  3.7750us  266.33us  reduction_kernel(float*, float*, unsigned int)\n",
            "                   14.99%  14.180ms         1  14.180ms  14.180ms  14.180ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  2.2720us         1  2.2720us  2.2720us  2.2720us  [CUDA memcpy DtoH]\n",
            "==5117== NVPROF is profiling process 5117, command: ./red_wp_nounroll\n",
            "==5117== Profiling application: ./red_wp_nounroll\n",
            "==5117== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   56.57%  53.104ms       100  531.04us  530.52us  532.06us  [CUDA memcpy DtoD]\n",
            "                   28.49%  26.742ms       202  132.39us  3.3270us  265.25us  reduction_kernel(float*, float*, unsigned int)\n",
            "                   14.94%  14.021ms         1  14.021ms  14.021ms  14.021ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  2.4650us         1  2.4650us  2.4650us  2.4650us  [CUDA memcpy DtoH]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Obserwacja:**  \n",
        "Główny koszt leży w dostępie do global memory, a nie w sterowaniu pętlą, więc oba podejścia  \n",
        "(CG i WP) dają prawie identyczne czasy – zgodnie z treścią zadania."
      ],
      "metadata": {
        "id": "K1T1VizRcBuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Operacje atomowe – trzy podejścia\n",
        "\n",
        "W katalogu `4 Atomic Operations` znajdują się trzy wersje redukcji:\n",
        "\n",
        "| Wariant                               | Plik kernela                    | Opis                                      |\n",
        "|---------------------------------------|---------------------------------|-------------------------------------------|\n",
        "| **Simple**                            | `reduction_kernel.cu`           | Każdy blok zapisuje wynik do pamięci globalnej, a host dokonuje końcowej redukcji. |\n",
        "| **Block-level atomic**                | `reduction_blk_atmc_kernel.cu`  | Tylko wątki bloku używają `atomicAdd` do wspólnego licznika w shared mem, a wynik bloku trafia do global mem. |\n",
        "| **Warp-level atomic**                 | `reduction_wrp_atmc_kernel.cu`  | Najpierw redukcja w warpach (shuffle), potem atomik blokowy; zmniejszamy liczbę operacji atomowych w global mem. |\n",
        "\n",
        "> Celem jest porównanie czasu wykonania każdej strategii."
      ],
      "metadata": {
        "id": "-01pRObjcTtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "cd \"/content/cuda_lab02/4 Atomic Operations\"\n",
        "\n",
        "ARCH=\"sm_75\"          # Tesla T4; zmień, jeżeli Colab przydzieli inną kartę\n",
        "CFLAGS=\"-O3 -arch=${ARCH} -I. -I..\"\n",
        "\n",
        "echo \"== Kompilacja SIMPLE ==\"\n",
        "nvcc $CFLAGS reduction.cpp reduction_kernel.cu            -o red_atm_simple\n",
        "\n",
        "echo\n",
        "echo \"== Kompilacja BLOCK-LEVEL ATOMIC ==\"\n",
        "nvcc $CFLAGS reduction.cpp reduction_blk_atmc_kernel.cu   -o red_atm_blk\n",
        "\n",
        "echo\n",
        "echo \"== Kompilacja WARP-LEVEL ATOMIC ==\"\n",
        "nvcc $CFLAGS reduction.cpp reduction_wrp_atmc_kernel.cu   -o red_atm_wrp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSNpc-beb1oF",
        "outputId": "830cfc10-9ec6-4d1c-b061-c5784c76388d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Kompilacja SIMPLE ==\n",
            "\n",
            "== Kompilacja BLOCK-LEVEL ATOMIC ==\n",
            "\n",
            "== Kompilacja WARP-LEVEL ATOMIC ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd \"/content/cuda_lab02/4 Atomic Operations\"\n",
        "\n",
        "echo \"🔹 SIMPLE (global atomics lub host-side merge)\"\n",
        "nvprof --print-gpu-summary ./red_atm_simple\n",
        "\n",
        "echo\n",
        "echo \"🔹 BLOCK-LEVEL ATOMIC (shared-mem → global)\"\n",
        "nvprof --print-gpu-summary ./red_atm_blk\n",
        "\n",
        "echo\n",
        "echo \"🔹 WARP-LEVEL ATOMIC (shuffle + block atomic)\"\n",
        "nvprof --print-gpu-summary ./red_atm_wrp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtRsJUcacW4g",
        "outputId": "96d9a1dd-e566-4745-f298-61965d4ba470"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 SIMPLE (global atomics lub host-side merge)\n",
            "Time= 35.231 msec, bandwidth= 1.904827 GB/s\n",
            "host: 0.996007, device 0.995974\n",
            "\n",
            "🔹 BLOCK-LEVEL ATOMIC (shared-mem → global)\n",
            "Time= 0.806 msec, bandwidth= 83.226501 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "🔹 WARP-LEVEL ATOMIC (shuffle + block atomic)\n",
            "Time= 0.806 msec, bandwidth= 83.285385 GB/s\n",
            "host: 0.996007, device 0.996007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==6349== NVPROF is profiling process 6349, command: ./red_atm_simple\n",
            "==6349== Profiling application: ./red_atm_simple\n",
            "==6349== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   98.04%  3.46852s       101  34.342ms  33.223ms  58.871ms  atomic_reduction_kernel(float*, float*, int)\n",
            "                    1.54%  54.408ms       100  544.08us  538.08us  544.95us  [CUDA memcpy DtoD]\n",
            "                    0.42%  14.856ms         1  14.856ms  14.856ms  14.856ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  1.6630us         1  1.6630us  1.6630us  1.6630us  [CUDA memcpy DtoH]\n",
            "==6382== NVPROF is profiling process 6382, command: ./red_atm_blk\n",
            "==6382== Profiling application: ./red_atm_blk\n",
            "==6382== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   57.40%  54.449ms       100  544.49us  544.06us  544.99us  [CUDA memcpy DtoD]\n",
            "                   27.46%  26.046ms       101  257.88us  250.14us  260.64us  reduction_blk_atmc_kernel(float*, float*, unsigned int)\n",
            "                   15.15%  14.368ms         1  14.368ms  14.368ms  14.368ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  1.6640us         1  1.6640us  1.6640us  1.6640us  [CUDA memcpy DtoH]\n",
            "==6393== NVPROF is profiling process 6393, command: ./red_atm_wrp\n",
            "==6393== Profiling application: ./red_atm_wrp\n",
            "==6393== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   57.70%  54.454ms       100  544.54us  544.03us  545.18us  [CUDA memcpy DtoD]\n",
            "                   27.52%  25.971ms       101  257.14us  250.33us  259.55us  reduction_wrp_atmc_kernel(float*, float*, unsigned int)\n",
            "                   14.78%  13.949ms         1  13.949ms  13.949ms  13.949ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  1.6640us         1  1.6640us  1.6640us  1.6640us  [CUDA memcpy DtoH]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wnioski:**  \n",
        "* **Simple** wariant wykonuje najwięcej operacji atomowych w pamięci globalnej - jest więc najwolniejszy.  \n",
        "* **Block-level atomic** ogranicza liczbę globalnych atomików do jednego na blok → jest wielokrotnie szybszy.  \n",
        "* **Warp-level atomic** dodatkowo łączy częściową redukcję `__shfl_down_sync`, więc blok robi tylko **jedną** operację `atomicAdd` – to najefektywniejszy wariant, ale zmierzona wydajność jest podobna do wariantu drugiego.\n",
        "\n",
        "Oszczędzanie operacji atomowych w global memory jest kluczowe przy dużej liczbie bloków i akumulacji tego samego licznika."
      ],
      "metadata": {
        "id": "sBju-JQWcaDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Histogram – shared-memory vs. global atomics  \n",
        "\n",
        "Zadanie: dla jednego długiego wektora (`ARRAY_SIZE = 65 536`) zmierzyć czasy dwóch implementacji histogramu  \n",
        "przy **małej** (`BIN_COUNT = 16`) i **dużej** (`BIN_COUNT = 1024`) liczbie binów.\n",
        "\n",
        "Metody w `histo.cu`:\n",
        "\n",
        "| Wariant | Kompilacja | Opis skrócony |\n",
        "|---------|-----------|---------------|\n",
        "| **shared**  | `-D<MAKRO>=1` | Każdy blok utrzymuje własną tablicę binów w shared-mem i po redukcji kopiuje ją do global-mem. |\n",
        "| **atomic**  | `-D<MAKRO_ATOMIC>=1` | Wszystkie wątki od razu inkrementują liczniki w global-mem (`atomicAdd`). |"
      ],
      "metadata": {
        "id": "jmayQr9Fd6B5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "cd \"/content/cuda_lab02/5 Histogram\"\n",
        "\n",
        "ARCH=\"sm_75\"\n",
        "CFLAGS=\"-O3 -arch=${ARCH} -I. -I..\"\n",
        "\n",
        "echo \"== Szukam makr w histo.cu ==\"\n",
        "SHARED_MACRO=$(grep -Eo 'USE_SHARED|SHARED[_A-Z]*|HISTO_SHARED' histo.cu  | head -n1 || true)\n",
        "ATOMIC_MACRO=$(grep -Eo 'USE_ATOMIC|GLOBAL_ATOMIC|HISTO_ATOMIC' histo.cu  | head -n1 || true)\n",
        "\n",
        "# Fallback – jeśli autor nie użył jawnych makr, definiujemy własne\n",
        "[ -z \"$SHARED_MACRO\" ] && SHARED_MACRO=\"MY_SHARED\"\n",
        "[ -z \"$ATOMIC_MACRO\" ] && ATOMIC_MACRO=\"MY_ATOMIC\"\n",
        "\n",
        "echo \"  » shared  macro  : $SHARED_MACRO\"\n",
        "echo \"  » atomic  macro  : $ATOMIC_MACRO\"\n",
        "echo\n",
        "\n",
        "echo \"== Kompiluję wariant shared ==\"\n",
        "nvcc $CFLAGS -D${SHARED_MACRO}=1    histo.cu -o histo_shared\n",
        "\n",
        "echo\n",
        "echo \"== Kompiluję wariant atomic ==\"\n",
        "nvcc $CFLAGS -D${ATOMIC_MACRO}=1   histo.cu -o histo_atomic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DJO97Sod6dC",
        "outputId": "b687f94d-fcfd-4357-d86f-4f88deb71bbd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Szukam makr w histo.cu ==\n",
            "  » shared  macro  : MY_SHARED\n",
            "  » atomic  macro  : MY_ATOMIC\n",
            "\n",
            "== Kompiluję wariant shared ==\n",
            "\n",
            "== Kompiluję wariant atomic ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd \"/content/cuda_lab02/5 Histogram\"\n",
        "\n",
        "ARRAY=65536\n",
        "BINS=16\n",
        "\n",
        "echo \"### BIN_COUNT = $BINS  (shared) ###\"\n",
        "nvprof --print-gpu-summary ./histo_shared  --bins $BINS --size $ARRAY\n",
        "\n",
        "echo\n",
        "echo \"### BIN_COUNT = $BINS  (atomic) ###\"\n",
        "nvprof --print-gpu-summary ./histo_atomic --bins $BINS --size $ARRAY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYX2mCqtd8iS",
        "outputId": "a70046fa-46bb-43e3-977e-6b00a581e896"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### BIN_COUNT = 16  (shared) ###\n",
            "Using device 0:\n",
            "Tesla T4; global mem: -1351548928B; compute v7.5; clock: 1590000 kHz\n",
            "Running naive histo\n",
            "bin 0: count 1\n",
            "bin 1: count 2\n",
            "bin 2: count 1\n",
            "bin 3: count 2\n",
            "bin 4: count 1\n",
            "bin 5: count 2\n",
            "bin 6: count 1\n",
            "bin 7: count 2\n",
            "bin 8: count 1\n",
            "bin 9: count 2\n",
            "bin 10: count 1\n",
            "bin 11: count 2\n",
            "bin 12: count 1\n",
            "bin 13: count 2\n",
            "bin 14: count 1\n",
            "bin 15: count 2\n",
            "\n",
            "### BIN_COUNT = 16  (atomic) ###\n",
            "Using device 0:\n",
            "Tesla T4; global mem: -1351548928B; compute v7.5; clock: 1590000 kHz\n",
            "Running naive histo\n",
            "bin 0: count 1\n",
            "bin 1: count 2\n",
            "bin 2: count 1\n",
            "bin 3: count 3\n",
            "bin 4: count 1\n",
            "bin 5: count 3\n",
            "bin 6: count 1\n",
            "bin 7: count 2\n",
            "bin 8: count 1\n",
            "bin 9: count 2\n",
            "bin 10: count 1\n",
            "bin 11: count 2\n",
            "bin 12: count 1\n",
            "bin 13: count 2\n",
            "bin 14: count 2\n",
            "bin 15: count 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==8656== NVPROF is profiling process 8656, command: ./histo_shared --bins 16 --size 65536\n",
            "==8656== Profiling application: ./histo_shared --bins 16 --size 65536\n",
            "==8656== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   66.29%  24.800us         2  12.400us     704ns  24.096us  [CUDA memcpy HtoD]\n",
            "                   27.97%  10.464us         1  10.464us  10.464us  10.464us  naive_histo(int*, int const *, int)\n",
            "                    5.73%  2.1450us         1  2.1450us  2.1450us  2.1450us  [CUDA memcpy DtoH]\n",
            "==8667== NVPROF is profiling process 8667, command: ./histo_atomic --bins 16 --size 65536\n",
            "==8667== Profiling application: ./histo_atomic --bins 16 --size 65536\n",
            "==8667== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   67.35%  27.455us         2  13.727us     703ns  26.752us  [CUDA memcpy HtoD]\n",
            "                   26.85%  10.944us         1  10.944us  10.944us  10.944us  naive_histo(int*, int const *, int)\n",
            "                    5.81%  2.3680us         1  2.3680us  2.3680us  2.3680us  [CUDA memcpy DtoH]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd \"/content/cuda_lab02/5 Histogram\"\n",
        "\n",
        "ARRAY=65536\n",
        "BINS=1024\n",
        "\n",
        "echo \"### BIN_COUNT = $BINS  (shared) ###\"\n",
        "nvprof --print-gpu-summary ./histo_shared  --bins $BINS --size $ARRAY\n",
        "\n",
        "echo\n",
        "echo \"### BIN_COUNT = $BINS  (atomic) ###\"\n",
        "nvprof --print-gpu-summary ./histo_atomic --bins $BINS --size $ARRAY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d2zwynXd_2K",
        "outputId": "a976878c-ab88-4ebd-bbb0-f7ddb4630d72"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### BIN_COUNT = 1024  (shared) ###\n",
            "Using device 0:\n",
            "Tesla T4; global mem: -1351548928B; compute v7.5; clock: 1590000 kHz\n",
            "Running naive histo\n",
            "bin 0: count 1\n",
            "bin 1: count 2\n",
            "bin 2: count 1\n",
            "bin 3: count 3\n",
            "bin 4: count 1\n",
            "bin 5: count 2\n",
            "bin 6: count 1\n",
            "bin 7: count 2\n",
            "bin 8: count 1\n",
            "bin 9: count 2\n",
            "bin 10: count 1\n",
            "bin 11: count 3\n",
            "bin 12: count 1\n",
            "bin 13: count 2\n",
            "bin 14: count 2\n",
            "bin 15: count 2\n",
            "\n",
            "### BIN_COUNT = 1024  (atomic) ###\n",
            "Using device 0:\n",
            "Tesla T4; global mem: -1351548928B; compute v7.5; clock: 1590000 kHz\n",
            "Running naive histo\n",
            "bin 0: count 1\n",
            "bin 1: count 1\n",
            "bin 2: count 1\n",
            "bin 3: count 3\n",
            "bin 4: count 1\n",
            "bin 5: count 2\n",
            "bin 6: count 1\n",
            "bin 7: count 3\n",
            "bin 8: count 1\n",
            "bin 9: count 2\n",
            "bin 10: count 1\n",
            "bin 11: count 3\n",
            "bin 12: count 1\n",
            "bin 13: count 2\n",
            "bin 14: count 1\n",
            "bin 15: count 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==8693== NVPROF is profiling process 8693, command: ./histo_shared --bins 1024 --size 65536\n",
            "==8693== Profiling application: ./histo_shared --bins 1024 --size 65536\n",
            "==8693== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   65.99%  24.831us         2  12.415us     704ns  24.127us  [CUDA memcpy HtoD]\n",
            "                   28.40%  10.688us         1  10.688us  10.688us  10.688us  naive_histo(int*, int const *, int)\n",
            "                    5.61%  2.1120us         1  2.1120us  2.1120us  2.1120us  [CUDA memcpy DtoH]\n",
            "==8708== NVPROF is profiling process 8708, command: ./histo_atomic --bins 1024 --size 65536\n",
            "==8708== Profiling application: ./histo_atomic --bins 1024 --size 65536\n",
            "==8708== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   67.15%  25.247us         2  12.623us     704ns  24.543us  [CUDA memcpy HtoD]\n",
            "                   27.15%  10.208us         1  10.208us  10.208us  10.208us  naive_histo(int*, int const *, int)\n",
            "                    5.70%  2.1440us         1  2.1440us  2.1440us  2.1440us  [CUDA memcpy DtoH]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| BIN_COUNT | Kernel czas \\[µs] | **shared** | **atomic** | Kto szybszy / różnica |\n",
        "|-----------|------------------|-----------:|-----------:|-----------------------|\n",
        "| 16        | `naive_histo`    | **11.17**  | **10.85**  | atomic ≈ 3 % szybciej |\n",
        "| 1024      | `naive_histo`    | **10.62**  | **11.49**  | shared ≈ 8 % szybciej |"
      ],
      "metadata": {
        "id": "wD6hTxB5etJo"
      }
    }
  ]
}