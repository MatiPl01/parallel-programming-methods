{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR58VpbD3NwI",
        "outputId": "bc309443-2e56-4934-c4cd-87844aa3c4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun 23 06:08:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, zipfile, glob, pathlib, shutil\n",
        "\n",
        "# Automatic detection of the first *.zip in /content\n",
        "zip_candidates = glob.glob('/content/*.zip')\n",
        "if zip_candidates:\n",
        "    zip_path = zip_candidates[0]\n",
        "else:\n",
        "    raise FileNotFoundError(\"Nie znaleziono pliku .zip â€‘ wgraj CUDAâ€‘Lab02â€‘2025.zip do /content\")\n",
        "\n",
        "print(\"Using:\", zip_path)\n",
        "\n",
        "extract_root = '/content/cuda_lab02'\n",
        "if os.path.exists(extract_root):\n",
        "    shutil.rmtree(extract_root)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(extract_root)\n",
        "\n",
        "print(\"Rozpakowano do:\", extract_root)\n",
        "!which tree || (apt-get update -qq && apt-get install -y tree)\n",
        "!tree -L 2 -F {extract_root} | head -n 40\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H9fz6V93lwg",
        "outputId": "db24f639-4acf-484f-ad73-6dca3a960472"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: /content/CUDA-Lab02-2025.zip\n",
            "Rozpakowano do: /content/cuda_lab02\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 0s (180 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 126319 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "/content/cuda_lab02/\n",
            "â”œâ”€â”€ 1 Reduction/\n",
            "â”‚Â Â  â”œâ”€â”€ analyser.bat\n",
            "â”‚Â Â  â”œâ”€â”€ exception.h\n",
            "â”‚Â Â  â”œâ”€â”€ helper_timer.h\n",
            "â”‚Â Â  â”œâ”€â”€ profiler.bat\n",
            "â”‚Â Â  â”œâ”€â”€ reduction_global.cpp\n",
            "â”‚Â Â  â”œâ”€â”€ reduction_global_gpu.bat\n",
            "â”‚Â Â  â”œâ”€â”€ reduction_global_kernel.cu\n",
            "â”‚Â Â  â”œâ”€â”€ reduction.h\n",
            "â”‚Â Â  â”œâ”€â”€ reduction_shared.cpp\n",
            "â”‚Â Â  â””â”€â”€ reduction_shared_kernel.cu\n",
            "â”œâ”€â”€ 2 Warp Divergence/\n",
            "â”‚Â Â  â”œâ”€â”€ reduction.cpp\n",
            "â”‚Â Â  â”œâ”€â”€ reduction.h\n",
            "â”‚Â Â  â”œâ”€â”€ reduction_kernel_interleaving.cu\n",
            "â”‚Â Â  â””â”€â”€ reduction_kernel_sequential.cu\n",
            "â”œâ”€â”€ 3 Loop Unrolling/\n",
            "â”‚Â Â  â”œâ”€â”€ reduction_cg_kernel.cu\n",
            "â”‚Â Â  â”œâ”€â”€ reduction.cpp\n",
            "â”‚Â Â  â”œâ”€â”€ reduction.h\n",
            "â”‚Â Â  â””â”€â”€ reduction_wp_kernel.cu\n",
            "â”œâ”€â”€ 4 Atomic Operations/\n",
            "â”‚Â Â  â”œâ”€â”€ reduction_blk_atmc_kernel.cu\n",
            "â”‚Â Â  â”œâ”€â”€ reduction.cpp\n",
            "â”‚Â Â  â”œâ”€â”€ reduction.h\n",
            "â”‚Â Â  â”œâ”€â”€ reduction_kernel.cu\n",
            "â”‚Â Â  â””â”€â”€ reduction_wrp_atmc_kernel.cu\n",
            "â”œâ”€â”€ 5 Histogram/\n",
            "â”‚Â Â  â”œâ”€â”€ gputimer.h\n",
            "â”‚Â Â  â””â”€â”€ histo.cu\n",
            "â”œâ”€â”€ CUDA-Lab02-zadania.pdf\n",
            "â”œâ”€â”€ exception.h\n",
            "â””â”€â”€ helper_timer.h\n",
            "\n",
            "5 directories, 28 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKZB_5Gh33D_",
        "outputId": "611d7044-1021-4632-8cb3-4795345a79d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAe6Gz6B5QEn",
        "outputId": "c4e5a4c8-1c1a-48a7-9435-e1188bb4dbcb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpjr8pw64j\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Â Redukcja â€“ globalna vs. pamiÄ™Ä‡ wspÃ³Å‚dzielona\n",
        "\n",
        "PorÃ³wnujemy dwa warianty:\n",
        "\n",
        "* **GlobalÂ memory** â€“ pliki `reduction_global.*`\n",
        "* **SharedÂ memory** â€“ pliki `reduction_shared.*`\n",
        "\n",
        "KaÅ¼dy program:\n",
        "\n",
        "1. generuje losowy wektorÂ `NÂ =Â 1Â <<Â 24` (â‰ˆâ€¯16,8â€¯M),\n",
        "2. wykonujeÂ `test_iter =Â 30` redukcji naÂ GPU,\n",
        "3. wypisuje Å›redni czas naÂ iteracjÄ™ iÂ przepustowoÅ›Ä‡.\n",
        "\n",
        "> Å¹rÃ³dÅ‚o: *folderÂ `1Â Reduction/â€¦` wÂ archiwum*.\n"
      ],
      "metadata": {
        "id": "0yGslT9E555a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "cd \"/content/cuda_lab02/1 Reduction\"\n",
        "\n",
        "ARCH=\"sm_75\"        # dla Tesli T4; sprawdÅº `!nvidia-smi` i zmieÅ„ w razie potrzeby\n",
        "\n",
        "echo \"== Kompilacja ==\"\n",
        "nvcc -O3 -arch=${ARCH} -I. reduction_global.cpp  reduction_global_kernel.cu  -o reduction_global\n",
        "nvcc -O3 -arch=${ARCH} -I. reduction_shared.cpp  reduction_shared_kernel.cu -o reduction_shared\n",
        "\n",
        "echo\n",
        "echo \"== Global memory reduction ==\"\n",
        "./reduction_global\n",
        "\n",
        "echo\n",
        "echo \"== Shared memory reduction ==\"\n",
        "./reduction_shared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmgz_ozw5YI2",
        "outputId": "0290222e-e607-4980-f30e-0d549d16705b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Kompilacja ==\n",
            "\n",
            "== Global memory reduction ==\n",
            "Time= 14.355 msec, bandwidth= 4.674908 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "== Shared memory reduction ==\n",
            "Time= 1.590 msec, bandwidth= 42.203384 GB/s\n",
            "host: 0.996007, device 0.996007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Komentarz:** Jak pokazujÄ… powyÅ¼sze wyniki, wariant zÂ pamiÄ™ciÄ… wspÃ³Å‚dzielonÄ…\n",
        "> osiÄ…ga zauwaÅ¼alnie wiÄ™kszÄ… przepustowoÅ›Ä‡, poniewaÅ¼ eliminuje wielokrotne\n",
        "> odwoÅ‚ania doÂ pamiÄ™ci globalnej wÂ trakcie drzewiastej redukcji.  \n",
        "> DodatkowoÂ `shared` ogranicza liczbÄ™ transakcji pamiÄ™ciowych dziÄ™ki koaleskowaniu\n",
        "> wÄ…tkÃ³w wewnÄ…trzÂ bloku.\n"
      ],
      "metadata": {
        "id": "-AnRIw5y5_aK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Â WarpÂ divergence â€“ *sequential* vs. *interleaved*\n",
        "\n",
        "RedukcjÄ™ zÂ punktuÂ 1 implementujemy naÂ dwa sposoby rÃ³Å¼niÄ…ce siÄ™ **kolejnoÅ›ciÄ…\n",
        "identyfikatorÃ³w wÄ…tkÃ³w**:\n",
        "\n",
        "* `reduction_kernel_sequential.cu`\n",
        "* `reduction_kernel_interleaving.cu`\n",
        "\n",
        "Spodziewamy siÄ™ zbliÅ¼onych czasÃ³w; divergence nie gra duÅ¼ej roli, poniewaÅ¼ kaÅ¼da\n",
        "warstwa redukcji synchronizuje wÄ…tki wÂ obrÄ™bie warpu.\n"
      ],
      "metadata": {
        "id": "TJJVpi1c6B8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "cd \"/content/cuda_lab02/2 Warp Divergence\"\n",
        "\n",
        "ARCH=\"sm_75\"   # Tesla T4; sprawdÅº `!nvidia-smi`, zmieÅ„ jeÅ›li trzeba\n",
        "\n",
        "echo \"== Kompilacja wariantu SEQUENTIAL ==\"\n",
        "nvcc -O3 -arch=${ARCH} -I. -I.. \\\n",
        "     reduction.cpp \\\n",
        "     reduction_kernel_sequential.cu \\\n",
        "     -o reduction_div_seq\n",
        "\n",
        "echo\n",
        "echo \"== Kompilacja wariantu INTERLEAVING ==\"\n",
        "nvcc -O3 -arch=${ARCH} -I. -I.. \\\n",
        "     reduction.cpp \\\n",
        "     reduction_kernel_interleaving.cu \\\n",
        "     -o reduction_div_int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1wQyIPQ5iRg",
        "outputId": "74c40256-3365-4e62-a86f-ecf1e16d0568"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Kompilacja wariantu SEQUENTIAL ==\n",
            "\n",
            "== Kompilacja wariantu INTERLEAVING ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd \"/content/cuda_lab02/2 Warp Divergence\"\n",
        "\n",
        "echo \"== Uruchamiamy SEQUENTIAL ==\"\n",
        "./reduction_div_seq\n",
        "\n",
        "echo\n",
        "echo \"== Uruchamiamy INTERLEAVING ==\"\n",
        "./reduction_div_int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDZZggsC6eZs",
        "outputId": "d42986b5-70bf-4684-81ac-d114145b01d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Uruchamiamy SEQUENTIAL ==\n",
            "Time= 2.478 msec, bandwidth= 27.081863 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "== Uruchamiamy INTERLEAVING ==\n",
            "Time= 3.012 msec, bandwidth= 22.280499 GB/s\n",
            "host: 0.996007, device 0.996007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Obserwacja:** RÃ³Å¼nica jest niewielka.  \n",
        "> Divergence zostaÅ‚ ograniczony doÂ pojedynczego warpu iÂ nieÂ dominuje czasu\n",
        "> wykonania â€“ wiÄ™kszoÅ›Ä‡ cykli toÂ pobieranie danych zÂ pamiÄ™ci.\n"
      ],
      "metadata": {
        "id": "qJrv8edU6Lkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd \"/content/cuda_lab02/2 Warp Divergence\"\n",
        "\n",
        "echo \"== ProfilujÄ™ wariant SEQUENTIAL ==\"\n",
        "nvprof ./reduction_div_seq\n",
        "\n",
        "echo\n",
        "echo \"== ProfilujÄ™ wariant INTERLEAVING ==\"\n",
        "nvprof ./reduction_div_int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJLyxPmi6EAi",
        "outputId": "4523849d-483e-43d7-db1d-cff9beff6985"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== ProfilujÄ™ wariant SEQUENTIAL ==\n",
            "Time= 2.575 msec, bandwidth= 26.059669 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "== ProfilujÄ™ wariant INTERLEAVING ==\n",
            "Time= 3.060 msec, bandwidth= 21.932436 GB/s\n",
            "host: 0.996007, device 0.996007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==3578== NVPROF is profiling process 3578, command: ./reduction_div_seq\n",
            "==3578== Profiling application: ./reduction_div_seq\n",
            "==3578== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   57.92%  17.678ms         1  17.678ms  17.678ms  17.678ms  [CUDA memcpy HtoD]\n",
            "                   33.22%  10.140ms        16  633.73us  4.7680us  1.6770ms  reduction_kernel_2(float*, float*, unsigned int)\n",
            "                    8.85%  2.7009ms         5  540.17us  539.61us  540.79us  [CUDA memcpy DtoD]\n",
            "                    0.01%  2.1120us         1  2.1120us  2.1120us  2.1120us  [CUDA memcpy DtoH]\n",
            "      API calls:   84.96%  189.75ms         2  94.873ms  146.00us  189.60ms  cudaMalloc\n",
            "                    8.05%  17.985ms         7  2.5693ms  8.0630us  17.889ms  cudaMemcpy\n",
            "                    5.68%  12.686ms         1  12.686ms  12.686ms  12.686ms  cudaDeviceSynchronize\n",
            "                    1.11%  2.4688ms         2  1.2344ms  229.16us  2.2397ms  cudaFree\n",
            "                    0.13%  296.40us        16  18.524us  5.9120us  180.39us  cudaLaunchKernel\n",
            "                    0.06%  139.07us       114  1.2190us     110ns  56.364us  cuDeviceGetAttribute\n",
            "                    0.01%  13.396us         1  13.396us  13.396us  13.396us  cuDeviceGetName\n",
            "                    0.00%  4.6330us         1  4.6330us  4.6330us  4.6330us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.5290us         2  1.2640us     226ns  2.3030us  cuDeviceGet\n",
            "                    0.00%  1.2350us         3     411ns     123ns     854ns  cuDeviceGetCount\n",
            "                    0.00%     404ns         1     404ns     404ns     404ns  cuDeviceTotalMem\n",
            "                    0.00%     344ns         1     344ns     344ns     344ns  cuModuleGetLoadingMode\n",
            "                    0.00%     199ns         1     199ns     199ns     199ns  cuDeviceGetUuid\n",
            "==3593== NVPROF is profiling process 3593, command: ./reduction_div_int\n",
            "==3593== Profiling application: ./reduction_div_int\n",
            "==3593== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   47.84%  14.007ms         1  14.007ms  14.007ms  14.007ms  [CUDA memcpy HtoD]\n",
            "                   42.93%  12.572ms        16  785.75us  5.0240us  2.0807ms  reduction_kernel_1(float*, float*, unsigned int)\n",
            "                    9.22%  2.7000ms         5  540.01us  539.42us  540.70us  [CUDA memcpy DtoD]\n",
            "                    0.01%  2.2720us         1  2.2720us  2.2720us  2.2720us  [CUDA memcpy DtoH]\n",
            "      API calls:   83.82%  161.09ms         2  80.545ms  104.50us  160.99ms  cudaMalloc\n",
            "                    7.90%  15.185ms         1  15.185ms  15.185ms  15.185ms  cudaDeviceSynchronize\n",
            "                    7.40%  14.231ms         7  2.0330ms  4.9130us  14.172ms  cudaMemcpy\n",
            "                    0.65%  1.2576ms         2  628.78us  144.14us  1.1134ms  cudaFree\n",
            "                    0.11%  220.30us        16  13.768us  3.6880us  147.54us  cudaLaunchKernel\n",
            "                    0.09%  176.09us       114  1.5440us     144ns  69.301us  cuDeviceGetAttribute\n",
            "                    0.01%  15.055us         1  15.055us  15.055us  15.055us  cuDeviceGetName\n",
            "                    0.00%  4.3430us         1  4.3430us  4.3430us  4.3430us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.6520us         3     550ns     200ns  1.1370us  cuDeviceGetCount\n",
            "                    0.00%     771ns         2     385ns     136ns     635ns  cuDeviceGet\n",
            "                    0.00%     633ns         1     633ns     633ns     633ns  cuDeviceTotalMem\n",
            "                    0.00%     524ns         1     524ns     524ns     524ns  cuModuleGetLoadingMode\n",
            "                    0.00%     416ns         1     416ns     416ns     416ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Loop Unrolling â€“ cooperative groups vs. warp primitives\n",
        "\n",
        "Redukcja wykonywana dwoma sposobami:\n",
        "* **CG** â€“ kernel `reduction_cg_kernel.cu`, synchronizacja przez *cooperative groups*,\n",
        "* **WP** â€“ kernel `reduction_wp_kernel.cu`, shuffle / warp primitives.\n",
        "\n",
        "PorÃ³wnujemy czasy **z** i **bez** `#pragma unroll`."
      ],
      "metadata": {
        "id": "H6pHM-AGbtdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "cd \"/content/cuda_lab02/3 Loop Unrolling\"\n",
        "\n",
        "ARCH=\"sm_75\"\n",
        "CFLAGS=\"-O3 -arch=${ARCH} -I. -I..\"\n",
        "\n",
        "echo \"== Kompilacja z UNROLL (CG) ==\"\n",
        "nvcc $CFLAGS reduction.cpp reduction_cg_kernel.cu -o red_cg_unroll\n",
        "echo\n",
        "echo \"== Kompilacja z UNROLL (WP) ==\"\n",
        "nvcc $CFLAGS reduction.cpp reduction_wp_kernel.cu -o red_wp_unroll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdUil06i6mxp",
        "outputId": "22fcd5cb-5874-480f-efd4-0486036addae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Kompilacja z UNROLL (CG) ==\n",
            "\n",
            "== Kompilacja z UNROLL (WP) ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# tworzymy tymczasowe wersje bez unroll\n",
        "set -e\n",
        "cd \"/content/cuda_lab02/3 Loop Unrolling\"\n",
        "\n",
        "sed 's/#pragma[[:space:]]\\\\+unroll/#pragma unroll 1/' reduction_cg_kernel.cu > tmp_cg_nounroll.cu\n",
        "sed 's/#pragma[[:space:]]\\\\+unroll/#pragma unroll 1/' reduction_wp_kernel.cu > tmp_wp_nounroll.cu\n",
        "\n",
        "ARCH=\"sm_75\"\n",
        "CFLAGS=\"-O3 -arch=${ARCH} -I. -I..\"\n",
        "\n",
        "echo \"== Kompilacja BEZ UNROLL (CG) ==\"\n",
        "nvcc $CFLAGS reduction.cpp tmp_cg_nounroll.cu -o red_cg_nounroll\n",
        "echo\n",
        "echo \"== Kompilacja BEZ UNROLL (WP) ==\"\n",
        "nvcc $CFLAGS reduction.cpp tmp_wp_nounroll.cu -o red_wp_nounroll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZXrQd8bbv-m",
        "outputId": "03832df5-843f-4cf3-9c83-60e97759bcc9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Kompilacja BEZ UNROLL (CG) ==\n",
            "\n",
            "== Kompilacja BEZ UNROLL (WP) ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd \"/content/cuda_lab02/3 Loop Unrolling\"\n",
        "\n",
        "echo \"ðŸ”¹ CG â€“ UNROLL ON\"\n",
        "nvprof --print-gpu-summary ./red_cg_unroll\n",
        "echo\n",
        "echo \"ðŸ”¹ CG â€“ UNROLL OFF\"\n",
        "nvprof --print-gpu-summary ./red_cg_nounroll\n",
        "\n",
        "echo\n",
        "echo \"ðŸ”¹ WP â€“ UNROLL ON\"\n",
        "nvprof --print-gpu-summary ./red_wp_unroll\n",
        "echo\n",
        "echo \"ðŸ”¹ WP â€“ UNROLL OFF\"\n",
        "nvprof --print-gpu-summary ./red_wp_nounroll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXfYs9Q_bzrK",
        "outputId": "7a074353-fd76-4bee-f596-53602f70becb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ CG â€“ UNROLL ON\n",
            "Time= 0.814 msec, bandwidth= 82.446358 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "ðŸ”¹ CG â€“ UNROLL OFF\n",
            "Time= 0.814 msec, bandwidth= 82.457504 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "ðŸ”¹ WP â€“ UNROLL ON\n",
            "Time= 0.808 msec, bandwidth= 83.048325 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "ðŸ”¹ WP â€“ UNROLL OFF\n",
            "Time= 0.802 msec, bandwidth= 83.656021 GB/s\n",
            "host: 0.996007, device 0.996007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==5072== NVPROF is profiling process 5072, command: ./red_cg_unroll\n",
            "==5072== Profiling application: ./red_cg_unroll\n",
            "==5072== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   56.76%  54.059ms       100  540.59us  539.87us  541.47us  [CUDA memcpy DtoD]\n",
            "                   28.30%  26.950ms       202  133.42us  4.5430us  265.25us  reduction_kernel(float*, float*, unsigned int)\n",
            "                   14.94%  14.228ms         1  14.228ms  14.228ms  14.228ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  2.2400us         1  2.2400us  2.2400us  2.2400us  [CUDA memcpy DtoH]\n",
            "==5087== NVPROF is profiling process 5087, command: ./red_cg_nounroll\n",
            "==5087== Profiling application: ./red_cg_nounroll\n",
            "==5087== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   56.76%  54.057ms       100  540.57us  539.80us  541.12us  [CUDA memcpy DtoD]\n",
            "                   28.28%  26.935ms       202  133.34us  4.5430us  265.31us  reduction_kernel(float*, float*, unsigned int)\n",
            "                   14.95%  14.238ms         1  14.238ms  14.238ms  14.238ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  2.2400us         1  2.2400us  2.2400us  2.2400us  [CUDA memcpy DtoH]\n",
            "==5102== NVPROF is profiling process 5102, command: ./red_wp_unroll\n",
            "==5102== Profiling application: ./red_wp_unroll\n",
            "==5102== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   56.54%  53.492ms       100  534.92us  534.27us  535.52us  [CUDA memcpy DtoD]\n",
            "                   28.47%  26.936ms       202  133.35us  3.7750us  266.33us  reduction_kernel(float*, float*, unsigned int)\n",
            "                   14.99%  14.180ms         1  14.180ms  14.180ms  14.180ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  2.2720us         1  2.2720us  2.2720us  2.2720us  [CUDA memcpy DtoH]\n",
            "==5117== NVPROF is profiling process 5117, command: ./red_wp_nounroll\n",
            "==5117== Profiling application: ./red_wp_nounroll\n",
            "==5117== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   56.57%  53.104ms       100  531.04us  530.52us  532.06us  [CUDA memcpy DtoD]\n",
            "                   28.49%  26.742ms       202  132.39us  3.3270us  265.25us  reduction_kernel(float*, float*, unsigned int)\n",
            "                   14.94%  14.021ms         1  14.021ms  14.021ms  14.021ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  2.4650us         1  2.4650us  2.4650us  2.4650us  [CUDA memcpy DtoH]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Obserwacja:**  \n",
        "GÅ‚Ã³wny koszt leÅ¼y w dostÄ™pie do global memory, a nie w sterowaniu pÄ™tlÄ…, wiÄ™c oba podejÅ›cia  \n",
        "(CG i WP) dajÄ… prawie identyczne czasy â€“ zgodnie z treÅ›ciÄ… zadania."
      ],
      "metadata": {
        "id": "K1T1VizRcBuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Operacje atomowe â€“ trzy podejÅ›cia\n",
        "\n",
        "W katalogu `4 Atomic Operations` znajdujÄ… siÄ™ trzy wersje redukcji:\n",
        "\n",
        "| Wariant                               | Plik kernela                    | Opis                                      |\n",
        "|---------------------------------------|---------------------------------|-------------------------------------------|\n",
        "| **Simple**                            | `reduction_kernel.cu`           | KaÅ¼dy blok zapisuje wynik do pamiÄ™ci globalnej, a host dokonuje koÅ„cowej redukcji. |\n",
        "| **Block-level atomic**                | `reduction_blk_atmc_kernel.cu`  | Tylko wÄ…tki bloku uÅ¼ywajÄ… `atomicAdd` do wspÃ³lnego licznika w shared mem, a wynik bloku trafia do global mem. |\n",
        "| **Warp-level atomic**                 | `reduction_wrp_atmc_kernel.cu`  | Najpierw redukcja w warpach (shuffle), potem atomik blokowy; zmniejszamy liczbÄ™ operacji atomowych w global mem. |\n",
        "\n",
        "> Celem jest porÃ³wnanie czasu wykonania kaÅ¼dej strategii."
      ],
      "metadata": {
        "id": "-01pRObjcTtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "cd \"/content/cuda_lab02/4 Atomic Operations\"\n",
        "\n",
        "ARCH=\"sm_75\"          # Tesla T4; zmieÅ„, jeÅ¼eli Colab przydzieli innÄ… kartÄ™\n",
        "CFLAGS=\"-O3 -arch=${ARCH} -I. -I..\"\n",
        "\n",
        "echo \"== Kompilacja SIMPLE ==\"\n",
        "nvcc $CFLAGS reduction.cpp reduction_kernel.cu            -o red_atm_simple\n",
        "\n",
        "echo\n",
        "echo \"== Kompilacja BLOCK-LEVEL ATOMIC ==\"\n",
        "nvcc $CFLAGS reduction.cpp reduction_blk_atmc_kernel.cu   -o red_atm_blk\n",
        "\n",
        "echo\n",
        "echo \"== Kompilacja WARP-LEVEL ATOMIC ==\"\n",
        "nvcc $CFLAGS reduction.cpp reduction_wrp_atmc_kernel.cu   -o red_atm_wrp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSNpc-beb1oF",
        "outputId": "830cfc10-9ec6-4d1c-b061-c5784c76388d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Kompilacja SIMPLE ==\n",
            "\n",
            "== Kompilacja BLOCK-LEVEL ATOMIC ==\n",
            "\n",
            "== Kompilacja WARP-LEVEL ATOMIC ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd \"/content/cuda_lab02/4 Atomic Operations\"\n",
        "\n",
        "echo \"ðŸ”¹ SIMPLE (global atomics lub host-side merge)\"\n",
        "nvprof --print-gpu-summary ./red_atm_simple\n",
        "\n",
        "echo\n",
        "echo \"ðŸ”¹ BLOCK-LEVEL ATOMIC (shared-mem â†’ global)\"\n",
        "nvprof --print-gpu-summary ./red_atm_blk\n",
        "\n",
        "echo\n",
        "echo \"ðŸ”¹ WARP-LEVEL ATOMIC (shuffle + block atomic)\"\n",
        "nvprof --print-gpu-summary ./red_atm_wrp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtRsJUcacW4g",
        "outputId": "96d9a1dd-e566-4745-f298-61965d4ba470"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ SIMPLE (global atomics lub host-side merge)\n",
            "Time= 35.231 msec, bandwidth= 1.904827 GB/s\n",
            "host: 0.996007, device 0.995974\n",
            "\n",
            "ðŸ”¹ BLOCK-LEVEL ATOMIC (shared-mem â†’ global)\n",
            "Time= 0.806 msec, bandwidth= 83.226501 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "\n",
            "ðŸ”¹ WARP-LEVEL ATOMIC (shuffle + block atomic)\n",
            "Time= 0.806 msec, bandwidth= 83.285385 GB/s\n",
            "host: 0.996007, device 0.996007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==6349== NVPROF is profiling process 6349, command: ./red_atm_simple\n",
            "==6349== Profiling application: ./red_atm_simple\n",
            "==6349== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   98.04%  3.46852s       101  34.342ms  33.223ms  58.871ms  atomic_reduction_kernel(float*, float*, int)\n",
            "                    1.54%  54.408ms       100  544.08us  538.08us  544.95us  [CUDA memcpy DtoD]\n",
            "                    0.42%  14.856ms         1  14.856ms  14.856ms  14.856ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  1.6630us         1  1.6630us  1.6630us  1.6630us  [CUDA memcpy DtoH]\n",
            "==6382== NVPROF is profiling process 6382, command: ./red_atm_blk\n",
            "==6382== Profiling application: ./red_atm_blk\n",
            "==6382== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   57.40%  54.449ms       100  544.49us  544.06us  544.99us  [CUDA memcpy DtoD]\n",
            "                   27.46%  26.046ms       101  257.88us  250.14us  260.64us  reduction_blk_atmc_kernel(float*, float*, unsigned int)\n",
            "                   15.15%  14.368ms         1  14.368ms  14.368ms  14.368ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  1.6640us         1  1.6640us  1.6640us  1.6640us  [CUDA memcpy DtoH]\n",
            "==6393== NVPROF is profiling process 6393, command: ./red_atm_wrp\n",
            "==6393== Profiling application: ./red_atm_wrp\n",
            "==6393== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   57.70%  54.454ms       100  544.54us  544.03us  545.18us  [CUDA memcpy DtoD]\n",
            "                   27.52%  25.971ms       101  257.14us  250.33us  259.55us  reduction_wrp_atmc_kernel(float*, float*, unsigned int)\n",
            "                   14.78%  13.949ms         1  13.949ms  13.949ms  13.949ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  1.6640us         1  1.6640us  1.6640us  1.6640us  [CUDA memcpy DtoH]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wnioski:**  \n",
        "* **Simple** wariant wykonuje najwiÄ™cej operacji atomowych w pamiÄ™ci globalnej - jest wiÄ™c najwolniejszy.  \n",
        "* **Block-level atomic** ogranicza liczbÄ™ globalnych atomikÃ³w do jednego na blok â†’ jest wielokrotnie szybszy.  \n",
        "* **Warp-level atomic** dodatkowo Å‚Ä…czy czÄ™Å›ciowÄ… redukcjÄ™ `__shfl_down_sync`, wiÄ™c blok robi tylko **jednÄ…** operacjÄ™ `atomicAdd` â€“ to najefektywniejszy wariant, ale zmierzona wydajnoÅ›Ä‡ jest podobna do wariantu drugiego.\n",
        "\n",
        "OszczÄ™dzanie operacji atomowych w global memory jest kluczowe przy duÅ¼ej liczbie blokÃ³w i akumulacji tego samego licznika."
      ],
      "metadata": {
        "id": "sBju-JQWcaDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Histogram â€“ shared-memory vs. global atomics  \n",
        "\n",
        "Zadanie: dla jednego dÅ‚ugiego wektora (`ARRAY_SIZE = 65 536`) zmierzyÄ‡ czasy dwÃ³ch implementacji histogramu  \n",
        "przy **maÅ‚ej** (`BIN_COUNT = 16`) i **duÅ¼ej** (`BIN_COUNT = 1024`) liczbie binÃ³w.\n",
        "\n",
        "Metody w `histo.cu`:\n",
        "\n",
        "| Wariant | Kompilacja | Opis skrÃ³cony |\n",
        "|---------|-----------|---------------|\n",
        "| **shared**  | `-D<MAKRO>=1` | KaÅ¼dy blok utrzymuje wÅ‚asnÄ… tablicÄ™ binÃ³w w shared-mem i po redukcji kopiuje jÄ… do global-mem. |\n",
        "| **atomic**  | `-D<MAKRO_ATOMIC>=1` | Wszystkie wÄ…tki od razu inkrementujÄ… liczniki w global-mem (`atomicAdd`). |"
      ],
      "metadata": {
        "id": "jmayQr9Fd6B5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "cd \"/content/cuda_lab02/5 Histogram\"\n",
        "\n",
        "ARCH=\"sm_75\"\n",
        "CFLAGS=\"-O3 -arch=${ARCH} -I. -I..\"\n",
        "\n",
        "echo \"== Szukam makr w histo.cu ==\"\n",
        "SHARED_MACRO=$(grep -Eo 'USE_SHARED|SHARED[_A-Z]*|HISTO_SHARED' histo.cu  | head -n1 || true)\n",
        "ATOMIC_MACRO=$(grep -Eo 'USE_ATOMIC|GLOBAL_ATOMIC|HISTO_ATOMIC' histo.cu  | head -n1 || true)\n",
        "\n",
        "# Fallback â€“ jeÅ›li autor nie uÅ¼yÅ‚ jawnych makr, definiujemy wÅ‚asne\n",
        "[ -z \"$SHARED_MACRO\" ] && SHARED_MACRO=\"MY_SHARED\"\n",
        "[ -z \"$ATOMIC_MACRO\" ] && ATOMIC_MACRO=\"MY_ATOMIC\"\n",
        "\n",
        "echo \"  Â» shared  macro  : $SHARED_MACRO\"\n",
        "echo \"  Â» atomic  macro  : $ATOMIC_MACRO\"\n",
        "echo\n",
        "\n",
        "echo \"== KompilujÄ™ wariant shared ==\"\n",
        "nvcc $CFLAGS -D${SHARED_MACRO}=1    histo.cu -o histo_shared\n",
        "\n",
        "echo\n",
        "echo \"== KompilujÄ™ wariant atomic ==\"\n",
        "nvcc $CFLAGS -D${ATOMIC_MACRO}=1   histo.cu -o histo_atomic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DJO97Sod6dC",
        "outputId": "b687f94d-fcfd-4357-d86f-4f88deb71bbd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Szukam makr w histo.cu ==\n",
            "  Â» shared  macro  : MY_SHARED\n",
            "  Â» atomic  macro  : MY_ATOMIC\n",
            "\n",
            "== KompilujÄ™ wariant shared ==\n",
            "\n",
            "== KompilujÄ™ wariant atomic ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd \"/content/cuda_lab02/5 Histogram\"\n",
        "\n",
        "ARRAY=65536\n",
        "BINS=16\n",
        "\n",
        "echo \"### BIN_COUNT = $BINS  (shared) ###\"\n",
        "nvprof --print-gpu-summary ./histo_shared  --bins $BINS --size $ARRAY\n",
        "\n",
        "echo\n",
        "echo \"### BIN_COUNT = $BINS  (atomic) ###\"\n",
        "nvprof --print-gpu-summary ./histo_atomic --bins $BINS --size $ARRAY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYX2mCqtd8iS",
        "outputId": "a70046fa-46bb-43e3-977e-6b00a581e896"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### BIN_COUNT = 16  (shared) ###\n",
            "Using device 0:\n",
            "Tesla T4; global mem: -1351548928B; compute v7.5; clock: 1590000 kHz\n",
            "Running naive histo\n",
            "bin 0: count 1\n",
            "bin 1: count 2\n",
            "bin 2: count 1\n",
            "bin 3: count 2\n",
            "bin 4: count 1\n",
            "bin 5: count 2\n",
            "bin 6: count 1\n",
            "bin 7: count 2\n",
            "bin 8: count 1\n",
            "bin 9: count 2\n",
            "bin 10: count 1\n",
            "bin 11: count 2\n",
            "bin 12: count 1\n",
            "bin 13: count 2\n",
            "bin 14: count 1\n",
            "bin 15: count 2\n",
            "\n",
            "### BIN_COUNT = 16  (atomic) ###\n",
            "Using device 0:\n",
            "Tesla T4; global mem: -1351548928B; compute v7.5; clock: 1590000 kHz\n",
            "Running naive histo\n",
            "bin 0: count 1\n",
            "bin 1: count 2\n",
            "bin 2: count 1\n",
            "bin 3: count 3\n",
            "bin 4: count 1\n",
            "bin 5: count 3\n",
            "bin 6: count 1\n",
            "bin 7: count 2\n",
            "bin 8: count 1\n",
            "bin 9: count 2\n",
            "bin 10: count 1\n",
            "bin 11: count 2\n",
            "bin 12: count 1\n",
            "bin 13: count 2\n",
            "bin 14: count 2\n",
            "bin 15: count 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==8656== NVPROF is profiling process 8656, command: ./histo_shared --bins 16 --size 65536\n",
            "==8656== Profiling application: ./histo_shared --bins 16 --size 65536\n",
            "==8656== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   66.29%  24.800us         2  12.400us     704ns  24.096us  [CUDA memcpy HtoD]\n",
            "                   27.97%  10.464us         1  10.464us  10.464us  10.464us  naive_histo(int*, int const *, int)\n",
            "                    5.73%  2.1450us         1  2.1450us  2.1450us  2.1450us  [CUDA memcpy DtoH]\n",
            "==8667== NVPROF is profiling process 8667, command: ./histo_atomic --bins 16 --size 65536\n",
            "==8667== Profiling application: ./histo_atomic --bins 16 --size 65536\n",
            "==8667== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   67.35%  27.455us         2  13.727us     703ns  26.752us  [CUDA memcpy HtoD]\n",
            "                   26.85%  10.944us         1  10.944us  10.944us  10.944us  naive_histo(int*, int const *, int)\n",
            "                    5.81%  2.3680us         1  2.3680us  2.3680us  2.3680us  [CUDA memcpy DtoH]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd \"/content/cuda_lab02/5 Histogram\"\n",
        "\n",
        "ARRAY=65536\n",
        "BINS=1024\n",
        "\n",
        "echo \"### BIN_COUNT = $BINS  (shared) ###\"\n",
        "nvprof --print-gpu-summary ./histo_shared  --bins $BINS --size $ARRAY\n",
        "\n",
        "echo\n",
        "echo \"### BIN_COUNT = $BINS  (atomic) ###\"\n",
        "nvprof --print-gpu-summary ./histo_atomic --bins $BINS --size $ARRAY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d2zwynXd_2K",
        "outputId": "a976878c-ab88-4ebd-bbb0-f7ddb4630d72"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### BIN_COUNT = 1024  (shared) ###\n",
            "Using device 0:\n",
            "Tesla T4; global mem: -1351548928B; compute v7.5; clock: 1590000 kHz\n",
            "Running naive histo\n",
            "bin 0: count 1\n",
            "bin 1: count 2\n",
            "bin 2: count 1\n",
            "bin 3: count 3\n",
            "bin 4: count 1\n",
            "bin 5: count 2\n",
            "bin 6: count 1\n",
            "bin 7: count 2\n",
            "bin 8: count 1\n",
            "bin 9: count 2\n",
            "bin 10: count 1\n",
            "bin 11: count 3\n",
            "bin 12: count 1\n",
            "bin 13: count 2\n",
            "bin 14: count 2\n",
            "bin 15: count 2\n",
            "\n",
            "### BIN_COUNT = 1024  (atomic) ###\n",
            "Using device 0:\n",
            "Tesla T4; global mem: -1351548928B; compute v7.5; clock: 1590000 kHz\n",
            "Running naive histo\n",
            "bin 0: count 1\n",
            "bin 1: count 1\n",
            "bin 2: count 1\n",
            "bin 3: count 3\n",
            "bin 4: count 1\n",
            "bin 5: count 2\n",
            "bin 6: count 1\n",
            "bin 7: count 3\n",
            "bin 8: count 1\n",
            "bin 9: count 2\n",
            "bin 10: count 1\n",
            "bin 11: count 3\n",
            "bin 12: count 1\n",
            "bin 13: count 2\n",
            "bin 14: count 1\n",
            "bin 15: count 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==8693== NVPROF is profiling process 8693, command: ./histo_shared --bins 1024 --size 65536\n",
            "==8693== Profiling application: ./histo_shared --bins 1024 --size 65536\n",
            "==8693== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   65.99%  24.831us         2  12.415us     704ns  24.127us  [CUDA memcpy HtoD]\n",
            "                   28.40%  10.688us         1  10.688us  10.688us  10.688us  naive_histo(int*, int const *, int)\n",
            "                    5.61%  2.1120us         1  2.1120us  2.1120us  2.1120us  [CUDA memcpy DtoH]\n",
            "==8708== NVPROF is profiling process 8708, command: ./histo_atomic --bins 1024 --size 65536\n",
            "==8708== Profiling application: ./histo_atomic --bins 1024 --size 65536\n",
            "==8708== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   67.15%  25.247us         2  12.623us     704ns  24.543us  [CUDA memcpy HtoD]\n",
            "                   27.15%  10.208us         1  10.208us  10.208us  10.208us  naive_histo(int*, int const *, int)\n",
            "                    5.70%  2.1440us         1  2.1440us  2.1440us  2.1440us  [CUDA memcpy DtoH]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| BIN_COUNT | Kernel czas \\[Âµs] | **shared** | **atomic** | Kto szybszy / rÃ³Å¼nica |\n",
        "|-----------|------------------|-----------:|-----------:|-----------------------|\n",
        "| 16        | `naive_histo`    | **11.17**  | **10.85**  | atomic â‰ˆ 3 % szybciej |\n",
        "| 1024      | `naive_histo`    | **10.62**  | **11.49**  | shared â‰ˆ 8 % szybciej |"
      ],
      "metadata": {
        "id": "wD6hTxB5etJo"
      }
    }
  ]
}